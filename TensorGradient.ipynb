{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \"\"\"docstring for tensor\"\"\"\n",
    "    def __init__(self, initial_value,op,graph):\n",
    "        self.initial_value = initial_value\n",
    "        self.graph = graph\n",
    "        self.op = op\n",
    "\n",
    "    def __add__(self, other): # change the + operateor's methods, other is another self \n",
    "        return self.graph.add(self, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseOp(object):\n",
    "    \"\"\"docstring for BaseOp\"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        self.inputs = [graph.convert(input_) for input_ in inputs]\n",
    "        self.output = graph.tensor(op=self)\n",
    "        self.graph  = graph\n",
    "\n",
    "    def compute(self,sess,*args):\n",
    "        raise NotImplementedError()\n",
    "    def gradient(self,grad):\n",
    "        raise NotImplementedError()\n",
    "'''\n",
    "class myParent( object ):\n",
    "    def __init__( self, customParam ):\n",
    "        self.parentNumber = 5\n",
    "        self.customParam = customParam\n",
    "\n",
    "class Child( myParent ):\n",
    "    def __init__( self, customParam ):\n",
    "        myParent.__init__( self, customParam )\n",
    "        self.childNumber = 4        \n",
    "'''        \n",
    "        \n",
    "class AddOp(BaseOp):\n",
    "\n",
    "    \n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0],inputs[1]),op =self) # build tensor with op=self\n",
    "        '''\n",
    "        print(\"11111111111 is :\"+str(inputs[0].initial_value))\n",
    "        if inputs[0] is not None and inputs[1] is not None:\n",
    "            self.output = graph.convert(   self.compute(inputs[0],inputs[1])   )\n",
    "        #self.output = graph.convert(1000000000000000000000000000000)\n",
    "    '''\n",
    "    def compute(self,a=0,b=0):\n",
    "        #a = self.inputs[0].initial_value\n",
    "        #b = self.inputs[1].initial_value\n",
    "        return a.initial_value + b.initial_value\n",
    "    def gradient(self,grad):\n",
    "        return [grad,grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "\t\"\"\"docstring for Graph\"\"\"\n",
    "\tdef tensor(self,initial_value=None,op=None):\n",
    "\t\treturn Tensor(initial_value=initial_value,op=op,graph=self)\n",
    "\n",
    "\tdef convert(self,value):\n",
    "\t\tif isinstance(value,Tensor):\n",
    "\t\t\treturn value\n",
    "\t\treturn self.tensor(initial_value=value)\n",
    "\n",
    "\tdef gradients(self,y,x_s):\n",
    "\n",
    "\t\t'''\n",
    "\t\t>>>queue.append(([1,1],1))\n",
    "\t\t>>>queue.pop(0)\n",
    "\t\t([1, 1], 1)\n",
    "\n",
    "\n",
    "\n",
    "    \tloss_op = graph.mean(graph.square(graph.transpose(y) - activations1))\n",
    "    \tparameters = [weights0, biases0, weights1, biases1]\n",
    "\n",
    "    \tgradients = graph.gradients(loss_op, parameters)\n",
    "\t\t'''\n",
    "\t\tqueue = []\n",
    "\t\tqueue.append((y,1)) #queue = [(y,1)]\n",
    "\n",
    "\t\tgrads = {}\n",
    "\n",
    "\t\twhile len(queue)>0:   #len(queue) = 1\n",
    "\t\t\ty,grad_y = queue.pop(0)  # y=y,grad_y =1\n",
    "\t\t\tgrad_y = self.convert(grad_y) #convert grad_y = 1 to a tensor = 1\n",
    "\t\t\t#print(y.op)\n",
    "\n",
    "\t\t\tgradients = y.op.gradient(grad_y) #gradients = [grad_y = 1,grad_y = 1] grad_y is a tensor\n",
    "\t\t\tassert len(gradients) == len(y.op.inputs)#len(gradients) = 2, len(y.op.inputs) = 2, \n",
    "            #c = a + b + a + a + b/ y = c /  y.op.inputs is [a + b + a + a ,b]\n",
    "\n",
    "\t\t\tfor tensor, gradient in zip(y.op.inputs, gradients):\n",
    "                #zip( [a + b + a + a ,b] , [grad_y = 1,grad_y = 1]  )\n",
    "\t\t\t\tif tensor in grads:  # 1: tensor = a + b + a + a , grads = {} \n",
    "                    #2:tensor = b, grads={a + b + a + a:grad_y}\n",
    "\t\t\t\t\tgrads[tensor] += gradient\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgrads[tensor] = gradient # 1 : grads[a + b + a + a] = gradient(=grad_y) \n",
    "                    #2:grads[b] = gradient(=grad_y)\n",
    "\n",
    "\t\t\t\tif tensor.op:\n",
    "\t\t\t\t\tqueue.append((tensor, gradient)) #queue=[(a + b + a + a,grad_y),(b,grad_y)]\n",
    "                    #**********************************************************\n",
    "                    # for b does not have a op, so tensor.op is None, so queue=[(a+b+a+a,grad_y)] \n",
    "                    # not have (b,grad_y)\n",
    "                    #**********************************************************\n",
    "                    \n",
    "                    \n",
    "            #while len(queue) = 2\n",
    "            #queue.pop(0) = (a + b + a + a,grad_y) => y = a + b + a + a ,grad_y=grad_y=1(tensor)\n",
    "            #......\n",
    "            #while len(queue) = 3\n",
    "            #queue.pop(0) = (b,grad_y) => y = b, grad_y = grad_y = 1\n",
    "            \n",
    "\n",
    "\t\treturn [grads[x] for x in x_s]\n",
    "\n",
    "\n",
    "\tdef add(self,a,b):\n",
    "\t\top = AddOp([a,b],graph=self)\n",
    "\t\treturn op.output_tensor_with_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Session(object):\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.state = {}\n",
    "        \n",
    "    def run_op(self, op, context):\n",
    "        args = [self.eval_tensor(tensor, context) for tensor in op.inputs]\n",
    "        return op.compute(self, *args)\n",
    "    \n",
    "    def eval_tensor(self, tensor, context):\n",
    "        \n",
    "        if tensor not in context:\n",
    "            if tensor.op is not None:\n",
    "                context[tensor] = self.run_op(tensor.op, context) #let context add the value in op.input\n",
    "            elif tensor in self.state and self.state[tensor] is not None:\n",
    "                context[tensor] = self.state[tensor]              #let context add the value in self.state\n",
    "            elif tensor not in self.state and tensor.initial_value is not None:\n",
    "                context[tensor] = self.state[tensor] = tensor.initial_value\n",
    "                                                        #let context add the value by the initial_value of tensor\n",
    "        return context[tensor]\n",
    "    def run(self, tensors, feed_dict=None):\n",
    "        \n",
    "        context = {}\n",
    "\n",
    "        if feed_dict:\n",
    "            context.update(feed_dict)\n",
    "\n",
    "        return [self.eval_tensor(tensor, context) for tensor in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "==================\n",
      "None\n",
      "None\n",
      "<__main__.AddOp object at 0x7f63cee4f450>\n",
      "==================\n",
      "Length is :2\n",
      "<__main__.AddOp object at 0x7f63cee4f590>\n",
      "None\n",
      "9\n",
      "5\n",
      "14\n",
      "14\n",
      "==================\n",
      "14\n",
      "==================\n",
      "2\n",
      "2\n",
      "==================\n",
      "c.op.inputs[0].op.inputs[0] = a + b + a\n",
      "7\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "graph = Graph()\n",
    "a = graph.tensor(2)\n",
    "b = graph.tensor(5)\n",
    "c = a + b + a + b\n",
    "print(c.initial_value)\n",
    "print(\"==================\")\n",
    "print(a.op)\n",
    "print(b.op)\n",
    "print(c.op)\n",
    "print(\"==================\")\n",
    "print(\"Length is :\"+str(len(c.op.inputs)))\n",
    "print(c.op.inputs[0].op)\n",
    "print(c.op.inputs[1].op)\n",
    "print(c.op.inputs[0].initial_value)\n",
    "print(c.op.inputs[1].initial_value)\n",
    "print(c.op.output.initial_value)\n",
    "print(c.op.output_with_op.initial_value)\n",
    "print(\"==================\")\n",
    "print(c.initial_value)\n",
    "print(\"==================\")\n",
    "grad_a, grad_b = graph.gradients(c, [a, b])\n",
    "print(grad_a.initial_value)\n",
    "print(grad_b.initial_value)\n",
    "print(\"==================\")\n",
    "print(\"c.op.inputs[0].op.inputs[0] = a + b + a\")\n",
    "print(c.op.inputs[0].op.inputs[0].initial_value)\n",
    "print(c.op.inputs[0].op.inputs[1].initial_value)\n",
    "#sess = Session(graph)\n",
    "#grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 1})\n",
    "#print(grad_a_)\n",
    "#print(grad_b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    `Tensor` represents a _value_ in the graph. It's just a data container with\n",
    "    methods for operator overloading (each of which delegate to the graph). It\n",
    "    includes:\n",
    "\n",
    "      - The initial value of the tensor.\n",
    "      - The operation which produced the tensor, if applicable.\n",
    "      - A reference to the graph this tensor belongs to.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_value, op, graph):\n",
    "        self.initial_value = initial_value\n",
    "        self.graph = graph\n",
    "        self.op = op\n",
    "\n",
    "    # ## [Operator Overloading](https://docs.python.org/2/reference/datamodel.html?highlight=__radd__#emulating-numeric-types)\n",
    "    def __add__(self, other):\n",
    "        return self.graph.add(self, other)\n",
    "\n",
    "class BaseOp(object):\n",
    "    \"\"\"\n",
    "    `BaseOp` represents an operation that performs computation on tensors.\n",
    "    Every operation consists of the following:\n",
    "\n",
    "      - A list of `inputs`, each converted to ensure they're all tensors.\n",
    "      - An output tensor to represent the result of the operation (which might\n",
    "        be `None`.)\n",
    "      - A reference to the graph so that each operation can generate new\n",
    "        operations when constructing gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, graph):\n",
    "        self.inputs = [graph.convert(input_) for input_ in inputs]\n",
    "        self.output = graph.tensor(op=self)\n",
    "        self.graph = graph\n",
    "\n",
    "    def compute(self, sess, *args):\n",
    "        \"\"\"\n",
    "        The `compute` method receives as input the _evaluated_ input tensors\n",
    "        and returns the result of performing its operation on the inputs.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        \"\"\"\n",
    "        The `gradient` method computes the partial derivative w.r.t. each input\n",
    "        to the operation. (Most of the derivatives come from\n",
    "        [Wikipedia](https://en.wikipedia.org/wiki/Differentiation_rules).)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class AddOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `AddOp` adds a tensor to another tensor. Uses the\n",
    "    [sum rule](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation) to\n",
    "    compute the partial derivatives.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, sess, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        return [grad, grad]\n",
    "    \n",
    "class Graph(object):\n",
    "    \"\"\"\n",
    "    `Graph` represents a computation to be evaluated by a `Session`. With the\n",
    "    exception of `Graph#tensor`, `Graph#convert`, and `Graph#gradients`, most\n",
    "    methods simply create an operation and return the output tensor of the\n",
    "    operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def tensor(self, initial_value=None, op=None):\n",
    "        \"\"\"\n",
    "        The `tensor` method defines a new tensor with the given initial value\n",
    "        and operation.\n",
    "        \"\"\"\n",
    "        return Tensor(initial_value=initial_value, graph=self, op=op)\n",
    "\n",
    "    def convert(self, value):\n",
    "        \"\"\"\n",
    "        The `convert` method returns the given value if it is a `Tensor`,\n",
    "        otherwise convert it to one.\n",
    "        \"\"\"\n",
    "        if isinstance(value, Tensor):\n",
    "            return value\n",
    "        return self.tensor(initial_value=value)\n",
    "\n",
    "    def add(self, a, b):\n",
    "        op = AddOp([a, b], graph=self)\n",
    "        return op.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
