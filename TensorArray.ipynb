{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \"\"\"docstring for tensor\"\"\"\n",
    "    def __init__(self, initial_value,op,graph):\n",
    "        self.initial_value = initial_value\n",
    "        self.graph = graph\n",
    "        self.op = op\n",
    "\n",
    "    def __add__(self, other): # change the + operateor's methods, other is another self \n",
    "        return self.graph.add(self, other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self.graph.neg(self)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self.graph.sub(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return self.graph.mul(self, other)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self.graph.div(self, other)\n",
    "    \n",
    "    \n",
    "    # ## [Reverse Operator Overloading](https://docs.python.org/2/reference/datamodel.html?highlight=__radd__#object.__radd__)\n",
    "    def __radd__(self, other):\n",
    "        return self.graph.add(other, self)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return self.graph.sub(other, self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.graph.mul(other, self)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return self.graph.div(other, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseOp(object):\n",
    "    \"\"\"docstring for BaseOp\"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        self.inputs = [graph.convert(input_) for input_ in inputs]\n",
    "        self.output = graph.tensor(op=self)\n",
    "        self.graph  = graph\n",
    "\n",
    "    def compute(self,sess,*args):\n",
    "        raise NotImplementedError()\n",
    "    def gradient(self,grad):\n",
    "        raise NotImplementedError()\n",
    "'''\n",
    "class myParent( object ):\n",
    "    def __init__( self, customParam ):\n",
    "        self.parentNumber = 5\n",
    "        self.customParam = customParam\n",
    "\n",
    "class Child( myParent ):\n",
    "    def __init__( self, customParam ):\n",
    "        myParent.__init__( self, customParam )\n",
    "        self.childNumber = 4        \n",
    "'''        \n",
    "        \n",
    "class AddOp(BaseOp):\n",
    "\n",
    "    \n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0],inputs[1]),op =self) # build tensor with op=self\n",
    "        '''\n",
    "        print(\"11111111111 is :\"+str(inputs[0].initial_value))\n",
    "        if inputs[0] is not None and inputs[1] is not None:\n",
    "            self.output = graph.convert(   self.compute(inputs[0],inputs[1])   )\n",
    "        #self.output = graph.convert(1000000000000000000000000000000)\n",
    "    '''\n",
    "    def compute(self,a=0,b=0):\n",
    "        a_ = graph.convert(a)\n",
    "        b_ = graph.convert(b)\n",
    "        return a_.initial_value + b_.initial_value\n",
    "    def gradient(self,grad):\n",
    "        return [grad,grad]\n",
    "    \n",
    "    \n",
    "    \n",
    "class NegOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `NegOp` negates a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0]),op =self) # build tensor with op=self\n",
    "    \n",
    "    def compute(self,  x):\n",
    "        x_ = graph.convert(a)\n",
    "        return -x_.initial_value\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        return [-grad]\n",
    "\n",
    "    \n",
    "class SubOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `SubOp` subtracts a tensor from another tensor. Also uses the\n",
    "    [sum rule](https://en.wikipedia.org/wiki/Sum_rule_in_differentiation) to\n",
    "    compute the partial derivatives.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0],inputs[1]),op =self) # build tensor with op=self\n",
    "    \n",
    "    \n",
    "    def compute(self, a, b):\n",
    "        a_ = graph.convert(a)\n",
    "        b_ = graph.convert(b)\n",
    "        return a_.initial_value - b_.initial_value\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        return [grad, -grad]\n",
    "    \n",
    "    \n",
    "class MulOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `MulOp` multiplies a tensor by another tensor. Uses the\n",
    "    [product rule](https://en.wikipedia.org/wiki/Product_rule) to compute the\n",
    "    partial derivatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0],inputs[1]),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self, a, b):\n",
    "        a_ = graph.convert(a)\n",
    "        b_ = graph.convert(b)\n",
    "        return a_.initial_value * b_.initial_value\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        a, b = self.inputs\n",
    "        return [grad * b, grad * a]\n",
    "    \n",
    "    \n",
    "class SquareOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `SquareOp` squares a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        self.output = graph.convert(   self.compute(inputs[0])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0]),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self,  x):\n",
    "        x_=graph.convert(x)\n",
    "        return np.square(x_.initial_value)\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        x = self.inputs[0].initial_value\n",
    "        return [grad * (2 * x)]\n",
    "    \n",
    "class DivOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `DivOp` divides a tensor by another tensor. Uses the\n",
    "    [quotient rule](https://en.wikipedia.org/wiki/Quotient_rule) to compute the\n",
    "    partial derivatives.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        #self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0],inputs[1]),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self, a, b):\n",
    "\n",
    "        a_ = graph.convert(a)\n",
    "        b_ = graph.convert(b)\n",
    "        return a_.initial_value/b_.initial_value\n",
    "    def gradient(self, grad):\n",
    "        a_, b_ = self.inputs\n",
    "        a=a_.initial_value\n",
    "        b=b_.initial_value\n",
    "        grad_=grad.initial_value\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(type(a))\n",
    "        print(type(b))\n",
    "        print(type(grad))\n",
    "        #grad_=self.graph.tensor(grad)\n",
    "        return [self.graph.tensor(grad_ / b), self.graph.tensor( grad_ * (-a / self.graph.square(b).initial_value)) ] \n",
    "\n",
    "    \n",
    "class TransposeOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `TransposeOp` tranposes a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        #self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0].initial_value),op =self) # build tensor with op=self\n",
    "\n",
    "    def compute(self, x):\n",
    "        x_=graph.convert(x)\n",
    "        return np.transpose(x_.initial_value)\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        return [self.graph.transpose(grad)]    \n",
    "    \n",
    "class DotOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `DotOp` computes the dot product between two tensors. Uses the\n",
    "    [product rule](https://en.wikipedia.org/wiki/Product_rule) to compute the\n",
    "    partial derivatives. Note that here we need to transpose the terms and\n",
    "    perform a dot product, assuming matrices rather than scalars.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        #self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0].initial_value,inputs[1].initial_value),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self,  a, b):\n",
    "        a_=graph.convert(a)\n",
    "        b_=graph.convert(b)\n",
    "        return np.dot(a_.initial_value, b_.initial_value)\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        a, b = self.inputs\n",
    "        aT = self.graph.transpose(a)\n",
    "        bT = self.graph.transpose(b)\n",
    "        return [\n",
    "            self.graph.dot(grad, bT),\n",
    "            self.graph.dot(aT, grad),\n",
    "        ]\n",
    "    \n",
    "class SigmoidOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `SigmoidOp` implements the\n",
    "    [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and its\n",
    "    derivative. Notice that the derivative uses the output of the operation\n",
    "    which saves recomputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        #self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0].initial_value),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self,  x):\n",
    "        x_=graph.convert(x)\n",
    "        return 1 / (1 + np.exp(-x_.initial_value))\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        y = self.output_tensor_with_op\n",
    "        return [grad * (y.initial_value * (1 - y.initial_value))]\n",
    "    \n",
    "    \n",
    "class MeanOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `MeanOp` computes the mean of a tensor. **Note** the gradient here is\n",
    "    intentially incorrect because computing it requires knowing the shape of\n",
    "    the input and output tensors. Fortunately, gradients are fairly malleable\n",
    "    in optimization.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, inputs,graph):\n",
    "        BaseOp.__init__(self, inputs,graph)\n",
    "        #self.output = graph.convert(   self.compute(inputs[0],inputs[1])   ) #build a tensor object but no op\n",
    "        self.output_tensor_with_op = graph.tensor(   self.compute(inputs[0].initial_value),op =self) # build tensor with op=self\n",
    "    \n",
    "\n",
    "    def compute(self, x):\n",
    "        x_=graph.convert(x)\n",
    "        return np.mean(x_.initial_value)\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        factor = 1\n",
    "        return [grad / factor]\n",
    "    \n",
    "class AssignOp(BaseOp):\n",
    "    \"\"\"\n",
    "    `AssignOp` updates the session's current state for a tensor. It is not\n",
    "    differentiable in this implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute(self, sess, a, b):\n",
    "        sess.state[self.inputs[0]] = b\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "\t\"\"\"docstring for Graph\"\"\"\n",
    "\tdef tensor(self,initial_value=None,op=None):\n",
    "\t\tif(Tensor==type(initial_value)):\n",
    "\t\t\treturn initial_value\n",
    "\t\telse:\n",
    "\t\t\treturn Tensor(initial_value=initial_value,op=op,graph=self)\n",
    "\n",
    "\tdef convert(self,value):\n",
    "\t\tif isinstance(value,Tensor):\n",
    "\t\t\treturn value\n",
    "\t\treturn self.tensor(initial_value=value)\n",
    "\n",
    "\tdef gradients(self,y,x_s):\n",
    "\n",
    "\t\t'''\n",
    "\t\t>>>queue.append(([1,1],1))\n",
    "\t\t>>>queue.pop(0)\n",
    "\t\t([1, 1], 1)\n",
    "\n",
    "\n",
    "\n",
    "    \tloss_op = graph.mean(graph.square(graph.transpose(y) - activations1))\n",
    "    \tparameters = [weights0, biases0, weights1, biases1]\n",
    "\n",
    "    \tgradients = graph.gradients(loss_op, parameters)\n",
    "\t\t'''\n",
    "\t\tqueue = []\n",
    "\t\tqueue.append((y,1)) #queue = [(y,1)]\n",
    "\n",
    "\t\tgrads = {}\n",
    "\n",
    "\t\twhile len(queue)>0:   #len(queue) = 1\n",
    "\t\t\ty,grad_y = queue.pop(0)  # y=y,grad_y =1\n",
    "\t\t\tgrad_y = self.convert(grad_y) #convert grad_y = 1 to a tensor = 1\n",
    "\t\t\t#print(y.op)\n",
    "\n",
    "\t\t\tgradients = y.op.gradient(grad_y) #gradients = [grad_y = 1,grad_y = 1] grad_y is a tensor\n",
    "\t\t\tassert len(gradients) == len(y.op.inputs)#len(gradients) = 2, len(y.op.inputs) = 2, \n",
    "            #c = a + b + a + a + b/ y = c /  y.op.inputs is [a + b + a + a ,b]\n",
    "\n",
    "\t\t\tfor tensor, gradient in zip(y.op.inputs, gradients):\n",
    "                #zip( [a + b + a + a ,b] , [grad_y = 1,grad_y = 1]  )\n",
    "\t\t\t\tif tensor in grads:  # 1: tensor = a + b + a + a , grads = {} \n",
    "                    #2:tensor = b, grads={a + b + a + a:grad_y}\n",
    "\t\t\t\t\tgrads[tensor] += gradient\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgrads[tensor] = gradient # 1 : grads[a + b + a + a] = gradient(=grad_y) \n",
    "                    #2:grads[b] = gradient(=grad_y)\n",
    "\n",
    "\t\t\t\tif tensor.op:\n",
    "\t\t\t\t\tqueue.append((tensor, gradient)) #queue=[(a + b + a + a,grad_y),(b,grad_y)]\n",
    "                    #**********************************************************\n",
    "                    # for b does not have a op, so tensor.op is None, so queue=[(a+b+a+a,grad_y)] \n",
    "                    # not have (b,grad_y)\n",
    "                    #**********************************************************\n",
    "                    \n",
    "                    \n",
    "            #while len(queue) = 2\n",
    "            #queue.pop(0) = (a + b + a + a,grad_y) => y = a + b + a + a ,grad_y=grad_y=1(tensor)\n",
    "            #......\n",
    "            #while len(queue) = 3\n",
    "            #queue.pop(0) = (b,grad_y) => y = b, grad_y = grad_y = 1\n",
    "            \n",
    "\n",
    "\t\treturn [grads[x] for x in x_s]\n",
    "\n",
    "\n",
    "\tdef add(self,a,b):\n",
    "\t\top = AddOp([a,b],graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef neg(self, x):\n",
    "\t\top = NegOp([x], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "    \n",
    "\tdef sub(self, a, b):\n",
    "\t\top = SubOp([a, b], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\n",
    "    \n",
    "\tdef mul(self, a, b):\n",
    "\t\top = MulOp([a , b], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef square(self, x):\n",
    "\t\top = SquareOp([x], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef div(self, a, b):\n",
    "\t\top = DivOp([a, b], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "\n",
    "\tdef transpose(self, x):\n",
    "\t\top = TransposeOp([x], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef dot(self, a, b):\n",
    "\t\top = DotOp([a, b], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef sigmoid(self, x):\n",
    "\t\top = SigmoidOp([x], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "\tdef mean(self, x):\n",
    "\t\top = MeanOp([x], graph=self)\n",
    "\t\treturn op.output_tensor_with_op\n",
    "    \n",
    "\tdef assign(self, a, b):\n",
    "\t\top = AssignOp([a, b], graph=self)\n",
    "\t\treturn op.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================OP\n",
      "None\n",
      "None\n",
      "<__main__.MeanOp object at 0x7f9c60648110>\n",
      "==================Initial value\n",
      "[<__main__.Tensor object at 0x7f9c605da350>]\n",
      "2.0\n",
      "==================\n",
      "2.0\n",
      "==================G\n",
      "1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "graph = Graph()\n",
    "a = graph.tensor(np.array([1,1,1,1,6]))\n",
    "b = graph.tensor(np.array([2,2,2,2,2]))\n",
    "c = graph.mean(a)\n",
    "print(\"==================OP\")\n",
    "print(a.op)\n",
    "print(b.op)\n",
    "print(c.op)\n",
    "print(\"==================Initial value\")\n",
    "#print(\"Length is :\"+str(len(c.op.inputs)))\n",
    "#print(c.op.inputs[0].op)\n",
    "#print(c.op.inputs[1].op)\n",
    "print(c.op.inputs)\n",
    "#print(c.op.inputs[1].initial_value)\n",
    "#print(c.op.output.initial_value)\n",
    "print(c.op.output_tensor_with_op.initial_value)\n",
    "print(\"==================\")\n",
    "print(c.initial_value)\n",
    "print(\"==================G\")\n",
    "#grad_a, grad_b = graph.gradients(c, [a, b])\n",
    "grad_a = graph.gradients(c, [a])\n",
    "print(grad_a[0].initial_value)\n",
    "#print(grad_a.initial_value)\n",
    "#print(grad_b.initial_value)\n",
    "print(\"==================\")\n",
    "#print(\"c.op.inputs[0].op.inputs[0] = a + b + a\")\n",
    "#print(c.op.inputs[0].op.inputs[0].initial_value)\n",
    "#print(c.op.inputs[0].op.inputs[1].initial_value)\n",
    "#sess = Session(graph)\n",
    "#grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 1})\n",
    "#print(grad_a_)\n",
    "#print(grad_b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "==================\n",
      "None\n",
      "None\n",
      "<__main__.AddOp object at 0x7f9c605f1890>\n",
      "==================\n",
      "Length is :2\n",
      "<__main__.MulOp object at 0x7f9c605f17d0>\n",
      "None\n",
      "16\n",
      "5\n",
      "21\n",
      "21\n",
      "==================\n",
      "21\n",
      "==================G\n",
      "32\n",
      "1\n",
      "==================\n",
      "c.op.inputs[0].op.inputs[0] = a + b + a\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "graph = Graph()\n",
    "a = graph.tensor(2)\n",
    "b = graph.tensor(5)\n",
    "c = a*a*a*a + b\n",
    "print(c.initial_value)\n",
    "print(\"==================\")\n",
    "print(a.op)\n",
    "print(b.op)\n",
    "print(c.op)\n",
    "print(\"==================\")\n",
    "print(\"Length is :\"+str(len(c.op.inputs)))\n",
    "print(c.op.inputs[0].op)\n",
    "print(c.op.inputs[1].op)\n",
    "print(c.op.inputs[0].initial_value)\n",
    "print(c.op.inputs[1].initial_value)\n",
    "print(c.op.output.initial_value)\n",
    "print(c.op.output_tensor_with_op.initial_value)\n",
    "print(\"==================\")\n",
    "print(c.initial_value)\n",
    "print(\"==================G\")\n",
    "grad_a, grad_b = graph.gradients(c, [a, b])\n",
    "print(grad_a.initial_value)\n",
    "print(grad_b.initial_value)\n",
    "print(\"==================\")\n",
    "print(\"c.op.inputs[0].op.inputs[0] = a + b + a\")\n",
    "print(c.op.inputs[0].op.inputs[0].initial_value)\n",
    "#print(c.op.inputs[0].op.inputs[1].initial_value)\n",
    "#sess = Session(graph)\n",
    "#grad_a_, grad_b_ = sess.run([grad_a, grad_b], feed_dict={a: 2, b: 1})\n",
    "#print(grad_a_)\n",
    "#print(grad_b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
